<!DOCTYPE html>
<html>
    <head>
        <link type = "text/css" rel="stylesheet" href = "stylesheet.css"/>
        <title>Ian Rankin</title>
    </head>
    <body>
        <div class = "main">
            <div class = "header">
                <ul>
                    <a class = "button" href="http://ianran.github.io/index"><li>Home</li></a>
                    <a class = "button" href="http://ianran.github.io/kinectRobot"><li>Kinect Robot</li></a>
                    <a class = "button" href="http://ianran.github.io/legoSwerve"><li>Lego Swerve</li></a>
                    <a class = "button" href="http://ianran.github.io/futureProjects"><li>Future Projects/Ideas</li></a>
                </ul>
            </div>
            
            
            
            <h1>Kinect Robot</h1>
            <h4><i>An Autonmous robot for finding paths automaticlly from depth image data from the Kinect.</i></h4>
            <h4 class = "workProgress">Project is a work in progress</h4>
            
            <img class = "SidePhoto" src = "http://ianran.github.io/IMG_2188.JPG"/>
            <p class = "side">
            The Kinect robot is a large project I am currently working on. The goal of the project is to
            complete a robot that can map out and avoid obstucules so that the robot can find it's way to the
            end target as fast as effciently as possible.
            </p>
            
            <p class = "side">
            The main data the robot will be using is a 
            <a href = "https://www.microsoft.com/en-us/kinectforwindows/meetkinect/default.aspx">Microsoft Kinect</a>.
            These sensors originaly designed for the xbox as a human input device are a very impressivse sensor for it's price.
            The Kinect gives back a depth image, which is a image that the pixels are a distance away from the camera rather
            then RGB data. These depth images are particuallry interesting, because from the location of an idividual pixel
            we can get the x and y angle the pixel is from the Kinect. Once you have 2 angles and the distance from the Kinect,
            it is easy to get a 3D point from every single pixel in the depth image.
            Once there is a 3D point for every pixel, the software will then go through an add each point to a 2D top view map of
            the suroundings of the robot so it can run the path-finding algorithems.
            </p>
            
            <h3>Drivetrain</h3>
            
            <p>
            The chassis of the robot is built out of  <a href = "http://www.vexrobotics.com/vex?ref=hometile">VEX Robotics</a> 
            parts. The drivetrain is a 4 wheeled skid drivetrain, where the lenght of the drivetrain is less then the width,
             which decreases the amount of sideways scrub on the wheels. Each side of the robot is chain driven off of one 
             <a href = "http://www.vexrobotics.com/wiki/3-Wire_Motor">VEX 3 wire motor</a>. Finally to read how far the
             robot has gone there is two
             <a href = "http://www.vexrobotics.com/vex/products/accessories/sensors/276-2156.html">VEX Encoders</a>, one
           	on each side of the drivetrain, these encoders are quadature.
            </p>
            
            <h3>Computing</h3>
            <img class = "SidePhoto" src = "http://ianran.github.io/image1.jpeg"/>
            
            <p class = "side">
            The main computing for the Kinect-Robot is done by a 
            <a href = "https://www.raspberrypi.org/products/raspberry-pi-2-model-b/">Raspberry Pi 2</a>
            . This takes sensor input from the kinect 
            over one of its USB ports. It also connects to a client from a seperate computer to recive commands from it over wifi.
            </p>
            
            <p class = "side">
            The Raspberry Pi then communicates with an 
            <a href = "">Ardiuno</a>
             also over USB using the UART interface to send info of where to go.
            The Ardiuno then sends data back saying where the robot has been. The Ardiuno gets data input from the 
            encoders and a 
            <a href = "http://www.adafruit.com/products/2472">9-DOF orientation sensor</a>
            to find where the robot has been. The processing of encoders and accelerometers take full advantage of
            the real-time computing of the Ardiuno, while also running at 5V that the encoders have as their output.
            The Ardiuno then has the PID Controllers and direct control over the motors sense they also want a 5V 
            <a href = "https://en.wikipedia.org/wiki/Servo_control">servo PWM</a>
            signal. I gave a link to more info about PWM because it is a common mistake to try to control servos using
            the duty cycle of a PWM signal rather then then pure width of pulses used for servo control.
            </p>
            
            <h3>Wiring</h3>
            <img class = "SidePhoto" src = "http://ianran.github.io/IMG_2192.JPG"/>
            
            <p class = "side">
            Image of the Adafruit 9-DOF Absolute orientation IMU fusion breakout - BN0055
            </p>
            
        </div>
    </body>
</html>

